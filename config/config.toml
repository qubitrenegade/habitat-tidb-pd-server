# PD Configuration.

name = "{{sys.hostname}}"
data-dir = "{{pkg.svc_data_path}}"

client-urls = "http://{{sys.ip}}:2379"
# if not set, use ${client-urls}
advertise-client-urls = ""

peer-urls = "http://{{sys.ip}}:2380"
# if not set, use ${peer-urls}
advertise-peer-urls = ""

initial-cluster = """
{{#eachAlive svc.members as |member| ~}}
  {{member.sys.hostname}}=http://{{member.sys.ip}}:2380{{~#unless @last}},{{/unless}}
{{/eachAlive ~}}
"""
initial-cluster-state = "new"

lease = 3
tso-save-interval = "3s"

namespace-classifier = "table"

{{#if cfg.security}}
[security]
# Path of file that contains list of trusted SSL CAs. if set, following four settings shouldn't be empty
cacert-path = ""
# Path of file that contains X509 certificate in PEM format.
cert-path = ""
# Path of file that contains X509 key in PEM format.
key-path = ""
{{/if}}

[log]
{{toToml cfg.log}}

# file logging
[log.file]
#filename = ""
# max log file size in MB
#max-size = 300
# max log file keep days
#max-days = 28
# maximum number of old log files to retain
#max-backups = 7
# rotate log by day
#log-rotate = true

[metric]
# prometheus client push interval, set "0s" to disable prometheus.
interval = "15s"
# prometheus pushgateway address, leaves it empty will disable prometheus.
address = ""

[schedule]
max-merge-region-size = 0
split-merge-interval = "1h"
max-snapshot-count = 3
max-pending-peer-count = 16
max-store-down-time = "30m"
leader-schedule-limit = 4
region-schedule-limit = 4
replica-schedule-limit = 8
merge-schedule-limit = 8
tolerant-size-ratio = 5.0

# customized schedulers, the format is as below
# if empty, it will use balance-leader, balance-region, hot-region as default
# [[schedule.schedulers]]
# type = "evict-leader"
# args = ["1"]

[replication]
# The number of replicas for each region.
max-replicas = 3
# The label keys specified the location of a store.
# The placement priorities is implied by the order of label keys.
# For example, ["zone", "rack"] means that we should place replicas to
# different zones first, then to different racks if we don't have enough zones.
location-labels = []

[label-property]
# Do not assign region leaders to stores that have these tags.
#  [[label-property.reject-leader]]
#  key = "zone"
#  value = "cn1
